{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "W3Iia0SPzBJm",
   "metadata": {
    "id": "W3Iia0SPzBJm",
    "tags": []
   },
   "source": [
    "# Prepare enviroment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BXPzE5df7wLB",
   "metadata": {
    "id": "BXPzE5df7wLB"
   },
   "source": [
    "We will use the Scikit-learn library as it will provide us with an interface to apply machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "FRfw5RCTIBDh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FRfw5RCTIBDh",
    "outputId": "fa046d3f-23c7-4f5c-bdb3-fc27eaeaeeaf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn==0.22 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (0.22)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn==0.22) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn==0.22) (1.3.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn==0.22) (1.10.0)\n"
     ]
    }
   ],
   "source": [
    "# we need to use an old version of scikit, otherwise we will face a compatiblity issue with eli5 (needed in order to use permutation importance\n",
    "# to answer research question #3\n",
    "# If we do not use this verion, eli5 will try to import a function (if_delegate_has_method) from scikit-learn that no longer exists\n",
    "# in recent versions of scikit-learn and we will run into an error\n",
    "!pip install scikit-learn==0.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "gYvVLwiqs0QU",
   "metadata": {
    "id": "gYvVLwiqs0QU",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: eli5 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (0.13.0)\n",
      "Requirement already satisfied: jinja2>=3.0.0 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from eli5) (3.1.2)\n",
      "Requirement already satisfied: attrs>17.1.0 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from eli5) (22.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from eli5) (1.19.5)\n",
      "Requirement already satisfied: scipy in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from eli5) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from eli5) (0.22)\n",
      "Requirement already satisfied: graphviz in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from eli5) (0.20.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from eli5) (0.9.0)\n",
      "Requirement already satisfied: six in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from eli5) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from jinja2>=3.0.0->eli5) (2.1.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/noor/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=0.20->eli5) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "_5T0lhe96KnQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_5T0lhe96KnQ",
    "outputId": "320900d6-1223-4e8d-a4dc-1c52a9b11325"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVR \u001b[38;5;66;03m# this will make a support vector machine for regression\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# permutation\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/__init__.py:74\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     77\u001b[0m     __check_build  \u001b[38;5;66;03m# avoid flakes unused variable error\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/base.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[1;32m     22\u001b[0m _DEFAULT_TAGS \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon_deterministic\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequires_positive_X\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_only\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequires_fit\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m}\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(estimator, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/sklearn/utils/__init__.py:18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmurmurhash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m murmurhash3_32\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/scipy/sparse/__init__.py:267\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_csc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/scipy/sparse/_csr.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[1;32m     11\u001b[0m                            get_csr_submatrix)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast, get_index_dtype\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "# model\n",
    "from sklearn.svm import SVR # this will make a support vector machine for regression\n",
    "\n",
    "# permutation\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance # the downside of SVM is that it is a 'black-box', this will allow us to access our model and compute feauture importance to answer research question #3 without having to resort to do more work to prepare a i.e., decision tree or bayesian network\n",
    "\n",
    "# scoring metric\n",
    "from sklearn.metrics import r2_score, mean_squared_error # to evaluate our SVM\n",
    "\n",
    "# formatting data\n",
    "from sklearn.preprocessing import scale # scale and center data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# model selection\n",
    "from sklearn.model_selection import train_test_split # split data into training and testing sets\n",
    "from sklearn.model_selection import GridSearchCV # this will do cross validation and allow us to find optimal parameters for our SVM\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold # provides K-Fold Cross Validation, in our case we will have 10 folds (9 subsamples will be used to train the model and the remaining 1 subsample is used to validate the model)\n",
    "\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be47c3fd-6ed6-4c71-91ba-5521f92d7d2b",
   "metadata": {
    "id": "be47c3fd-6ed6-4c71-91ba-5521f92d7d2b"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.19.5.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Image\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# data wrangling\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pearsonr\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/pandas/compat/__init__.py:25\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     IS64,\n\u001b[1;32m     19\u001b[0m     PY39,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     PYPY,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     is_numpy_dev,\n\u001b[1;32m     27\u001b[0m     np_version_under1p21,\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n",
      "File \u001b[0;32m~/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages/pandas/compat/numpy/__init__.py:25\u001b[0m\n\u001b[1;32m     21\u001b[0m     np_percentile_argname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _nlv \u001b[38;5;241m<\u001b[39m Version(_min_numpy_ver):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis version of pandas is incompatible with numpy < \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour numpy version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_np_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upgrade numpy to >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to use this pandas version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_np_version\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_numpy_dev\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.19.5.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "# visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "from seaborn import histplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "515d6993-9dd4-4ead-8646-ee7b585714ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5574898d-b74f-4f87-b2a9-d2171fd536d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d460cd8d-807f-4d2a-bac6-18a3a9fca83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b23e722-0898-4df0-95ab-e06b46367f32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.8.16\n",
      "numpy version: 1.19.5\n",
      "scipy version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "print('Python version:', platform.python_version())\n",
    "print('numpy version:', np.__version__)\n",
    "print('scipy version:', scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_pdbRUevTEdy",
   "metadata": {
    "id": "_pdbRUevTEdy",
    "tags": []
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ScrbBosq_eL_",
   "metadata": {
    "id": "ScrbBosq_eL_"
   },
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vUZpzGHm7L-a",
   "metadata": {
    "id": "vUZpzGHm7L-a"
   },
   "outputs": [],
   "source": [
    "df_RAW_pre2023 = pd.read_csv('pre-2023.csv')\n",
    "df_RAW_2023 = pd.read_csv('2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EIAPootfsFwF",
   "metadata": {
    "id": "EIAPootfsFwF"
   },
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8X2R17x81ymj",
   "metadata": {
    "id": "8X2R17x81ymj"
   },
   "source": [
    "### Removing columns we dont care about"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W1kbXJODaJou",
   "metadata": {
    "id": "W1kbXJODaJou"
   },
   "source": [
    "Let's remove the columns with variables that are not present in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "htWEBAxqt7kP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htWEBAxqt7kP",
    "outputId": "881f62e1-6b06-4178-827e-a2a71dc757aa"
   },
   "outputs": [],
   "source": [
    "print(df_RAW_pre2023.columns)\n",
    "print(df_RAW_2023.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mqGul3KweWfW",
   "metadata": {
    "id": "mqGul3KweWfW"
   },
   "outputs": [],
   "source": [
    "remove_pre2023 = ['Positive affect', 'Negative affect']\n",
    "\n",
    "df_RAW_pre2023 = df_RAW_pre2023.drop(columns=remove_pre2023)\n",
    "\n",
    "remove_2023 = ['Standard error of ladder score', 'upperwhisker', 'lowerwhisker',\n",
    "              'Ladder score in Dystopia', 'Explained by: Log GDP per capita',\n",
    "              'Explained by: Social support','Explained by: Healthy life expectancy',\n",
    "              'Explained by: Freedom to make life choices','Explained by: Generosity',\n",
    "              'Explained by: Perceptions of corruption','Dystopia + residual']\n",
    "\n",
    "df_RAW_2023 = df_RAW_2023.drop(columns=remove_2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ObYtJMygxUUd",
   "metadata": {
    "id": "ObYtJMygxUUd"
   },
   "source": [
    "Add a `year` column to `df_2023` so that information is not lost if the dataframe is merged with `df_pre2023` in the future,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MbOGk6OYxR4D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "MbOGk6OYxR4D",
    "outputId": "766fd629-7435-446c-d0e3-20a0cbbb57bf"
   },
   "outputs": [],
   "source": [
    "df_RAW_2023.insert(loc=1, column='year', value=2023)\n",
    "df_RAW_2023.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ugBkiR49yf2n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugBkiR49yf2n",
    "outputId": "a5b4f974-616c-47a1-a343-28d8fde2184a"
   },
   "outputs": [],
   "source": [
    "print(df_RAW_pre2023.columns)\n",
    "print(df_RAW_2023.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w8dR7rRgdmeU",
   "metadata": {
    "id": "w8dR7rRgdmeU"
   },
   "source": [
    "Sanity check: Let's make sure all the dataframes have the same number of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35vZg5IetIa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d35vZg5IetIa",
    "outputId": "133ad730-f7c7-4341-ec02-0f325f333cae"
   },
   "outputs": [],
   "source": [
    "# List of DataFrames\n",
    "dataframes = [df_RAW_pre2023, df_RAW_2023]\n",
    "\n",
    "# Count number of columns in each DataFrame\n",
    "for i, df in enumerate(dataframes):\n",
    "    num_columns = df.shape[1]\n",
    "    print(\"Number of columns in DataFrame\", i+1, \":\", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pAnG2TrK0KL1",
   "metadata": {
    "id": "pAnG2TrK0KL1"
   },
   "source": [
    "### Standardize naming scheme for dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Te8V_D750h53",
   "metadata": {
    "id": "Te8V_D750h53"
   },
   "source": [
    "Between the two datasets, there is a slight difference in the naming scheme. We the column names to be the same to allow for merging later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lDMjT7lU008c",
   "metadata": {
    "id": "lDMjT7lU008c"
   },
   "outputs": [],
   "source": [
    "df_RAW_pre2023.rename(columns={\n",
    "    'Country name': 'country',\n",
    "    'year': 'year',\n",
    "    'Life Ladder': 'happiness',\n",
    "    'Log GDP per capita': 'GDP',\n",
    "    'Social support' : 'socialSupport',\n",
    "    'Healthy life expectancy at birth': 'lifeExpectancy',\n",
    "    'Freedom to make life choices': 'freedom',\n",
    "    'Generosity': 'generosity',\n",
    "    'Perceptions of corruption' : 'corruption',\n",
    "}, inplace=True)\n",
    "\n",
    "df_RAW_2023.rename(columns={\n",
    "    'Country name': 'country',\n",
    "    'year': 'year',\n",
    "    'Ladder score': 'happiness',\n",
    "    'Logged GDP per capita': 'GDP',\n",
    "    'Social support' : 'socialSupport',\n",
    "    'Healthy life expectancy': 'lifeExpectancy',\n",
    "    'Freedom to make life choices': 'freedom',\n",
    "    'Generosity': 'generosity',\n",
    "    'Perceptions of corruption' : 'corruption',\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "K_Y29XLE0dDL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K_Y29XLE0dDL",
    "outputId": "f3efa03d-5fb9-4b27-b0bc-ab71964ce0e0"
   },
   "outputs": [],
   "source": [
    "print(df_RAW_pre2023.columns)\n",
    "print(df_RAW_2023.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0hBdHNYxzyyc",
   "metadata": {
    "id": "0hBdHNYxzyyc"
   },
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZugGNAiSz1k6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ZugGNAiSz1k6",
    "outputId": "90bfc4e9-39e1-4827-ffde-e02d46dc5904"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_RAW_pre2023, df_RAW_2023], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sktrs2kpsIIH",
   "metadata": {
    "id": "sktrs2kpsIIH"
   },
   "source": [
    "### Dealing with NaN or 0 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FE1AGJjznnz3",
   "metadata": {
    "id": "FE1AGJjznnz3"
   },
   "source": [
    "Let's make sure we do not have any NA values, since Scikit-learn's support vector machines do not work with datasets with missing values.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KEyO5saQoGbv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEyO5saQoGbv",
    "outputId": "1784b8c3-86e8-48e9-c5ca-39202ef7020c"
   },
   "outputs": [],
   "source": [
    "df.dtypes # check if there are character-based place holders for missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E4SJELdlon9I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4SJELdlon9I",
    "outputId": "dbe365ef-411a-40dd-ecf3-55830bd9061f"
   },
   "outputs": [],
   "source": [
    "print(\"Number of null or missing values: \",df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g3JGlf46n1ce",
   "metadata": {
    "id": "g3JGlf46n1ce"
   },
   "source": [
    "According to the [statistics appendix](https://s3.amazonaws.com/happiness-report/2018/Appendix1ofChapter2.pdf), 0 is not a meaningful score for any of the variables. Any cells that have a value of zero are actually cells with missing data.\n",
    "\n",
    "Let's check to see if any observation has a 0 as a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jx4a6LNJn79I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jx4a6LNJn79I",
    "outputId": "e9ecff99-9f85-4365-8070-73f3c518f1d0"
   },
   "outputs": [],
   "source": [
    "zero_indices = np.where(df == 0)\n",
    "num_zeros = zero_indices[0].shape[0]\n",
    "print(f'Number of cells with a value of zero: {num_zeros}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3o0sNCUQ7N1L",
   "metadata": {
    "id": "3o0sNCUQ7N1L"
   },
   "source": [
    "While it seems that some countries have missing data, it is not justifiable to discard the whole observation. In our dataset, usually for country that has missing data, it is only empty for a single variable and still has the 6 other dimensions filled. Therefore, discarding rows with missing information is not a good tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2WO9lawB62cM",
   "metadata": {
    "id": "2WO9lawB62cM"
   },
   "source": [
    "Even though some cells have a value of `0` or `NaN`, we can use **interpolation** to estimate a resonable value for these cells and replace these cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thvio4Rw79Pe",
   "metadata": {
    "id": "thvio4Rw79Pe"
   },
   "source": [
    "A linear method of interpolation will draw a straight line between the points surrounding the missing value(s) and use this line to estimate a reasonable value. It's worth noting that we will assumes that the change between the values is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79deL4w1rjP_",
   "metadata": {
    "id": "79deL4w1rjP_"
   },
   "source": [
    "Interpolation works only on cells exclusively with `NaN`. As such, we will replace all 0's with NaN's before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hev2C4DxrzoW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hev2C4DxrzoW",
    "outputId": "2dd76b5f-d2d3-4d01-dbfd-9a8c9f3ab7be"
   },
   "outputs": [],
   "source": [
    "df.replace(0, np.nan)\n",
    "print(\"Number of null or missing values: \",df.isnull().sum().sum()) # check to see if we successfully replaced 0's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1i7PNF5f8dAj",
   "metadata": {
    "id": "1i7PNF5f8dAj"
   },
   "source": [
    "Now that our dataset exclusively has missing data as `NaN`, we can proceed with the interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "omA37kc6pRZ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "omA37kc6pRZ2",
    "outputId": "f3de71fe-57d9-440c-c0c6-67d54182fa9f"
   },
   "outputs": [],
   "source": [
    "df.interpolate(method='linear', inplace=True)\n",
    "print(\"Number of null or missing values: \",df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70_mvu3n8oXq",
   "metadata": {
    "id": "70_mvu3n8oXq"
   },
   "source": [
    "Success! We have no more `NaN` values in out dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "goPu6_nMsJ6p",
   "metadata": {
    "id": "goPu6_nMsJ6p"
   },
   "source": [
    "How many years does our dataframe account for? which countries are missing from each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gyeNsbzDsqmE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyeNsbzDsqmE",
    "outputId": "0b6ea8ab-f39a-4c11-b720-36f64db3edb2"
   },
   "outputs": [],
   "source": [
    "# To identify the countries in df\n",
    "all_countries = df['country'].unique()\n",
    "print(f\"The dataframe covers the years: {all_countries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qY7Mp8H3sZx-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qY7Mp8H3sZx-",
    "outputId": "64ae7863-8a79-4f6d-8b54-c9814a912690"
   },
   "outputs": [],
   "source": [
    "# To identify the years in df\n",
    "years = df['year'].unique()\n",
    "print(f\"The dataframe covers the years: {years}\")\n",
    "\n",
    "# To find out which countries are missing for each year:\n",
    "for year in years:\n",
    "    countries_in_year = df[df['year'] == year]['country'].unique()\n",
    "    missing_countries = set(all_countries) - set(countries_in_year)\n",
    "    print(f\"In the year {year}, the following countries are missing: {missing_countries}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TSPgsuMQss14",
   "metadata": {
    "id": "TSPgsuMQss14",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **Q1:** Can we predict this year's (2023) happiness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OowZtYLWHqXM",
   "metadata": {
    "id": "OowZtYLWHqXM"
   },
   "source": [
    "Happiness is considered an indicator of societal health. If a government can predict its citizens' happiness accurately, it can use that information to evaluate the success of its current policies and make adjustments where necessary.\n",
    "\n",
    "We already have the 2023 data. What we can do is train a model on the pre-2023 data to see if the predicted 2023 scores actually match the actual ones. If the model we build has a high accuracy, we can forecast 2024 scores!\n",
    "\n",
    "Our ultimate goal is to predict the 2023 data, and were interested in seeing how well our model performs specifically on this year, we wil exclude the 2023 data from our training and testing sets entirely. We will then train our model on the pre-2023 data, and once the model is trained, use it to make predictions on the 2023 data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jBbBbWQ0Ym_-",
   "metadata": {
    "id": "jBbBbWQ0Ym_-"
   },
   "source": [
    "## Pre-processing for this specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xzKjT8RPHu60",
   "metadata": {
    "id": "xzKjT8RPHu60"
   },
   "outputs": [],
   "source": [
    "df_pre2023 = df[df['year'] < 2023] # we will train and test our model on this dataframe\n",
    "df_2023 = df[df['year'] == 2023] # well compare predicted data with this dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jm0bmDKJK_TJ",
   "metadata": {
    "id": "Jm0bmDKJK_TJ"
   },
   "source": [
    "**Why did we choose SVM?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JaENk7ZzSqZW",
   "metadata": {
    "id": "JaENk7ZzSqZW"
   },
   "source": [
    "### Dealing with countries present in pre-2023 data, but not 2023 (or vice versa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "StbsC32cSxQZ",
   "metadata": {
    "id": "StbsC32cSxQZ"
   },
   "source": [
    "Some countries present in your training data are not present in the 2023 data, which cause an issue in our model later on as it expects to see the same features (in this case, countries) in both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BgYmuRgTWNOV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BgYmuRgTWNOV",
    "outputId": "ab2bf648-f1a3-4a41-b52e-7c39c2c3d1e4"
   },
   "outputs": [],
   "source": [
    "# Get a list of all unique countries in each dataframe\n",
    "countries_pre2023 = set(df_pre2023['country'].unique())\n",
    "countries_2023 = set(df_2023['country'].unique())\n",
    "\n",
    "# Find countries present in pre-2023 data but not 2023\n",
    "missing_in_2023 = countries_pre2023 - countries_2023\n",
    "print(\"Countries present in pre-2023 data but not 2023:\")\n",
    "print(missing_in_2023)\n",
    "\n",
    "# Find countries present in 2023 data but not pre-2023\n",
    "missing_in_pre2023 = countries_2023 - countries_pre2023\n",
    "print(\"\\nCountries present in 2023 data but not pre-2023:\")\n",
    "print(missing_in_pre2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3euV25oqWS_t",
   "metadata": {
    "id": "3euV25oqWS_t"
   },
   "source": [
    "It turns out the countries the 2023 dataframe is missing are:\n",
    "\n",
    " 'Azerbaijan', 'Turkmenistan', 'Qatar', 'Angola', 'Belize', 'Oman', 'Belarus', 'Bhutan', 'Libya', 'Kuwait', 'Trinidad and Tobago', 'Djibouti', 'Maldives', 'Cuba', 'Haiti', 'Rwanda', 'South Sudan', 'Syria', 'Central African Republic', 'Eswatini', 'Somaliland region', 'Guyana', 'Yemen', 'Suriname', 'Burundi', 'Lesotho', 'Sudan', 'Somalia'\n",
    "\n",
    "Hence, we need to remove these countries from the pre-2023 dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fej_P6uSXozo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fej_P6uSXozo",
    "outputId": "1e2c0db0-81bb-4e70-e79a-d2898cdb2606"
   },
   "outputs": [],
   "source": [
    "df_pre2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OqYbK0OeWySz",
   "metadata": {
    "id": "OqYbK0OeWySz"
   },
   "outputs": [],
   "source": [
    "remove_countries = ['Azerbaijan', 'Turkmenistan', 'Qatar', 'Angola', 'Belize',\n",
    "                    'Oman', 'Belarus', 'Bhutan', 'Libya', 'Kuwait', 'Trinidad and Tobago',\n",
    "                    'Djibouti', 'Maldives', 'Cuba', 'Haiti', 'Rwanda', 'South Sudan',\n",
    "                    'Syria', 'Central African Republic', 'Eswatini', 'Somaliland region',\n",
    "                    'Guyana', 'Yemen', 'Suriname', 'Burundi', 'Lesotho', 'Sudan', 'Somalia']\n",
    "\n",
    "# Filter the dataframe so only countries we want are left\n",
    "df_pre2023 = df_pre2023[~df_pre2023 ['country'].isin(remove_countries)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uqfuthBnIQAf",
   "metadata": {
    "id": "uqfuthBnIQAf"
   },
   "source": [
    "### Model formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quqBwjXp9rEi",
   "metadata": {
    "id": "quqBwjXp9rEi"
   },
   "source": [
    "To avoid data leakage, we need to split variables into indepdent and dependent variables then perform one-hot encoding. We also need to split our data into training and test sets before scale our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SMbeAUf69rEi",
   "metadata": {
    "id": "SMbeAUf69rEi"
   },
   "source": [
    "### Split the data into dependent and independent variables\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7oDsOycs9rEi",
   "metadata": {
    "id": "7oDsOycs9rEi"
   },
   "source": [
    "Let's format the data so we can be ready to create a support vector machine.\n",
    "\n",
    "In this case, the variable we want to predict is data from the 'happiness' coloumn. We will store this in **y**.\n",
    "\n",
    "The remaining other coloumns will correspond to our classification variables. This data will be store in **X**.\n",
    "\n",
    "So the independent variables are: `'country', 'year', 'GDP', 'socialSupport',\n",
    "       'lifeExpectancy', 'freedom', 'generosity', 'corruption'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24nC_3hg9rEi",
   "metadata": {
    "id": "24nC_3hg9rEi"
   },
   "outputs": [],
   "source": [
    "# we will make a copy of our df so as to not modify the original.\n",
    "\n",
    "X = df_pre2023.drop('happiness', axis=1).copy()\n",
    "X_2023 = df_2023.drop('happiness', axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apE6wvUL9rEj",
   "metadata": {
    "id": "apE6wvUL9rEj"
   },
   "outputs": [],
   "source": [
    "y = df_pre2023['happiness'].copy()\n",
    "y_2023 = df_2023['happiness'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rZ6yS7kJ9rEj",
   "metadata": {
    "id": "rZ6yS7kJ9rEj"
   },
   "source": [
    "### One-hot encoding **EDIT COMMENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uSvi3Goy9rEj",
   "metadata": {
    "id": "uSvi3Goy9rEj"
   },
   "source": [
    "In our datasets, we have one categorical column; country. We can't just plug it into our SVM as is, because SVM requires numerical input.\n",
    "\n",
    "Luckily we can use a trick to bypass this issue by using one-Hot encoding.\n",
    "\n",
    "One-hot encoding will create a binary column for each category in the feature. The only downside is that the dimensionality of out data will increase since we have 195 countries in the world. Even though we could maintain dimensionality by using ordinal encoding, we will not use it as it would assign a value to each country, implying a ranking--which is not true in our case. It would, for an example, cluster country 193, 194, 195 together because it would assume they are similiar.\n",
    "\n",
    "This type of data pre-procesing needs to happen after data splitting to prevent \"data leakage\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9-9FZnTk9rEj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "9-9FZnTk9rEj",
    "outputId": "2afbc00f-b2a7-4389-8818-30705be1ab0d"
   },
   "outputs": [],
   "source": [
    "X_encoded = pd.get_dummies(X, columns=['country'])\n",
    "X_encoded_2023 = pd.get_dummies(X_2023, columns=['country'])\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zyLryd1p9rEj",
   "metadata": {
    "id": "zyLryd1p9rEj"
   },
   "source": [
    "You can see that the outcome of hot-encoding is that, for example, the row corresponding to the UAE has a 0 for all country columns (i.e., country_Afghanistan, country_Albania, country_Algeria) except for the column country_United_Arab_Emirates--this is how we maintain the country information for each column in a numerical format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aS7BuBuusxpa",
   "metadata": {
    "id": "aS7BuBuusxpa"
   },
   "source": [
    "Since our model performed really well at predicting this year, lets forcast the happiness of contries in the upcoming year.\n",
    "\n",
    "**check which countries have missing years** Even though the survey claims to have included data about 156 countries, there are only a small number of countries for which data has been recorded in the initial years.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T2sanB9S9rEk",
   "metadata": {
    "id": "T2sanB9S9rEk"
   },
   "source": [
    "### Splitting into training and test set & scaling **EDIT COMMENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SGY__N5E9rEk",
   "metadata": {
    "id": "SGY__N5E9rEk"
   },
   "source": [
    "NOTE: Another reason why we already split the dataset into a training and test dataset is to avoid data leakage while scaling. Data leakage in this case means that the information about the training set corrupts the testing one.\n",
    "\n",
    "K-Fold Cross Validation is a popular resampling technique used to evaluate models on a limited data sample. The primary goal of cross-validation is to prevent overfitting, but it also gives insight into how the model will generalize to an independent dataset.\n",
    "\n",
    "In K-Fold Cross Validation, the original sample is randomly partitioned into k equal sized subsamples (or \"folds\"). Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k-1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation.\n",
    "\n",
    "When k equals 10 (i.e., 10-fold cross validation), this means that the original sample is randomly divided into 10 subsamples. For each of 10 iterations, 9 subsamples are used to train the model and the remaining 1 subsample is used to validate the model (i.e., assess its performance). The final model performance is then typically the average performance across the 10 iterations.\n",
    "\n",
    "This method is particularly useful when you have a small dataset because it maximizes both the test and training data. However, it also has a higher computational cost as the model needs to be trained k times.\n",
    "\n",
    "Another variation of K-Fold Cross Validation is Stratified K-Fold Cross Validation where each set contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds. This is generally a better option as it takes imbalance in the data distribution into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639F2EJl9rEk",
   "metadata": {
    "id": "639F2EJl9rEk"
   },
   "source": [
    "The `random_state` parameter is a way to set the seed for the pseudo-random number generator used in the algorithm, particularly for methods that involve random processes such as random subsampling, randomly initializing parameters, etc.\n",
    "\n",
    "The actual number you use as a seed doesn't matter so much. What's important is that the same seed will result in the same random sequence, providing reproducibility for your analysis.\n",
    "\n",
    "So if you set `random_state` to a high number, it won't affect the quality of your model or its ability to generalize. It will just start the random number generator at a different point in its sequence compared to if you used a lower seed. But as long as you use the same seed each time (whatever that number may be), you'll get the same results each time you run your code.\n",
    "\n",
    "If you don't set `random_state`, each time you run your code you may get different results, because the seed for the random number generator will be set randomly (often using the current time), causing your code to start at a different point in the random sequence each time it's run.\n",
    "\n",
    "The most important consideration with `random_state` is to set it the same way each time if you want reproducible results, not whether the seed is a small or large number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aZn1AMf9rEk",
   "metadata": {
    "id": "8aZn1AMf9rEk"
   },
   "source": [
    "Creating a \"fold\" in the context of k-fold cross-validation refers to splitting the data into several subsets of roughly equal size.\n",
    "\n",
    "For instance, when you perform 10-fold cross-validation, you divide your dataset into 10 \"folds\" or subsets. If you have 1000 samples in your dataset, each fold will have approximately 100 samples.\n",
    "\n",
    "For each round of cross-validation, one of these folds is used as the testing set and the remaining folds are combined to form the training set. The model is trained on the training set and evaluated on the testing set.\n",
    "\n",
    "This process is repeated 10 times (or however many folds you have), each time with a different fold being used as the test set. This way, each sample in your dataset is included in the test set exactly once.\n",
    "\n",
    "The advantage of k-fold cross-validation is that it allows you to use all of your data for both training and testing, which can lead to more robust estimates of the model's performance.\n",
    "\n",
    "Here's an example with 5-fold cross-validation:\n",
    "\n",
    "- Round 1: Train on folds 2, 3, 4, 5; test on fold 1\n",
    "- Round 2: Train on folds 1, 3, 4, 5; test on fold 2\n",
    "- Round 3: Train on folds 1, 2, 4, 5; test on fold 3\n",
    "- Round 4: Train on folds 1, 2, 3, 5; test on fold 4\n",
    "- Round 5: Train on folds 1, 2, 3, 4; test on fold 5\n",
    "\n",
    "Each \"fold\" is used once as the test set, and the model's performance is averaged across the 5 rounds to get the final model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B2qzoNzc9rEk",
   "metadata": {
    "id": "B2qzoNzc9rEk"
   },
   "outputs": [],
   "source": [
    "# lets create folds and tell the KFold function whether or not to shuffle the data beforehand.\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zD1IQH5W9rEk",
   "metadata": {
    "id": "zD1IQH5W9rEk"
   },
   "source": [
    "The Radial Basis Function (RBF) kernel we are using for our Support Vector Machine assumes that the data is centered and scaled. The RBF kernel measures the similarity between samples based on the Euclidean distance, and if the data is not scaled, features with larger scales can dominate the distance computation and have a disproportionate impact on the model.\n",
    "\n",
    "The scaling process involves subtracting the mean of each feature and dividing it by the standard deviation. In other words, each column should have a mean value of 0 and a standard deviation of 1. We need to do this in both the training and testing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F5SU35OY9rEk",
   "metadata": {
    "id": "F5SU35OY9rEk"
   },
   "source": [
    "Scaling data is a form of preprocessing, and generally, preprocessing steps should be performed within each fold of the cross-validation process. If you perform scaling before the cross-validation, the scaling parameters (mean, standard deviation for standard scaling, for instance) would be computed using all of the data, including the test data that's supposed to be \"unseen\". This could lead to information leakage from the test set into the training set, which may lead to overly optimistic performance metrics.\n",
    "\n",
    "The correct way is to compute the scaling parameters (i.e., fit the scaler) on the training set of each fold, then apply those parameters to scale the data in both the training and test sets.\n",
    "\n",
    "Here's a general procedure using `StandardScaler` and `KFold` in scikit-learn as an example:\n",
    "\n",
    "\n",
    "In this way, each fold gets its own \"view\" of the world that is not contaminated by the test data.\n",
    "\n",
    "This process becomes easier when you use `Pipeline` in scikit-learn, which allows you to define a pipeline of transformations that end with a classifier or regressor. When using `cross_val_score` or other similar functions with a pipeline, the fitting of the preprocessing steps will automatically happen on the training data of each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pBCFH1oH9rEk",
   "metadata": {
    "id": "pBCFH1oH9rEk"
   },
   "outputs": [],
   "source": [
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Prepare empty lists to hold the training and testing data for each fold\n",
    "X_train_list, X_test_list = [], []\n",
    "y_train_list, y_test_list = [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X_encoded):\n",
    "    # Split the data\n",
    "    X_train, X_test = X_encoded.iloc[train_index], X_encoded.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the scaler on the training set and transform both sets\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Append to the lists\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bmF6pM8vL3VJ",
   "metadata": {
    "id": "bmF6pM8vL3VJ"
   },
   "source": [
    "## Initializing support vector machine **EDIT COMMENTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0Akpa14L3VK",
   "metadata": {
    "id": "a0Akpa14L3VK"
   },
   "source": [
    "Our problem is a regression one, not a classification one since we are working with continuous target values (e.g., predicting a numerical value, in our case, happiness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eSLUCHY8L3VK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSLUCHY8L3VK",
    "outputId": "3f630943-4455-40a9-ba7d-531cbaafe6d9"
   },
   "outputs": [],
   "source": [
    "# Initialize the SVR model\n",
    "model = SVR()\n",
    "\n",
    "# Create an empty list to store scores for each fold\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 0.1  # 10% acceptable limit of error (the percentage of the true value that you'll allow as error)\n",
    "\n",
    "for i in range(kf.get_n_splits()):\n",
    "    # Get the data for this fold\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "\n",
    "    # Fit the model and make predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and store the performance metrics\n",
    "    r2_scores.append(r2_score(y_test, y_pred))\n",
    "    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    # computing accuracy\n",
    "    relative_error = np.abs((y_test - y_pred) / y_test) # Compute the relative error\n",
    "    correct_predictions = np.sum(relative_error <= threshold) # Count the number of predictions that fall within the threshold\n",
    "    accuracy_scores.append(correct_predictions / len(y_test)) # Compute and store the accuracy for this fold\n",
    "\n",
    "# Print average performance metrics\n",
    "print(\"Average R^2 Score:\", np.mean(r2_scores))\n",
    "print(\"Average RMSE:\", np.mean(rmse_scores))\n",
    "print('Average Accuracy: ', np.mean(accuracy_scores)) # Print average accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTWIAry5MkxG",
   "metadata": {
    "id": "kTWIAry5MkxG"
   },
   "source": [
    "So far we defined and fit the SVM regression model within each fold on the pre-2023 data. Currently, out of the box, our SVM model performed pretty well:\n",
    "\n",
    " R^2 = `0.889`, RMSE = `0.372`, Accuracy = `85.1%`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fpFRRvIL3VK",
   "metadata": {
    "id": "0fpFRRvIL3VK"
   },
   "source": [
    "Accuracy: If the difference between the predicted and actual values falls within this threshold, then the prediction is deemed correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7Thkno7L3VK",
   "metadata": {
    "id": "b7Thkno7L3VK"
   },
   "source": [
    "Computing averages for the performance metrics across the multiple folds of a cross-validation process gives us an overall understanding of the model's performance, while reducing the variance and bias that might come from a single split of the data.\n",
    "\n",
    "Here are some more detailed reasons:\n",
    "\n",
    "1. **Generalization**: The average performance metrics provide a more general evaluation of the model's ability to predict new, unseen data. Since in k-fold cross-validation each fold serves as a test set once, we are essentially testing the model's performance on the entire dataset, which should give us a good estimate of how well the model will perform on new data.\n",
    "\n",
    "2. **Stability**: Averaging metrics over multiple folds helps to ensure that the evaluation is stable and reliable, not overly dependent on a particular random split of the data. The variance of the estimate decreases as the number of folds increases.\n",
    "\n",
    "3. **Bias/Variance Trade-off**: Cross-validation allows us to balance bias and variance. A model that performs well on average across many different splits of the data is less likely to be suffering from high bias (underfitting) or high variance (overfitting), as both of these issues would likely cause poor performance on at least some of the folds.\n",
    "\n",
    "So, the averaging process in k-fold cross-validation serves to give a more reliable and less biased estimate of the model's true performance than would be obtained from a single train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "klWfhzl1MDEL",
   "metadata": {
    "id": "klWfhzl1MDEL"
   },
   "source": [
    "## Tune/optimize model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fBoUyg9tMDEX",
   "metadata": {
    "id": "fBoUyg9tMDEX"
   },
   "source": [
    "Though our model perfromed quite well, let's find the optimal values for our model paramerts to see if we can improve performance.\n",
    "\n",
    "\n",
    "So let's use cross validation with the hopes of improving the accuracy on the **testing dataset** from the pre-2023 data.\n",
    "\n",
    "Since we have more than one parameter to optimize, we will use GridSearchCV() to test all possible combinations.\n",
    "\n",
    "**Epsilon**: SVR uses the epsilon parameter to specify the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.\n",
    "**C**: Regularization parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CR7086hORBq2",
   "metadata": {
    "id": "CR7086hORBq2"
   },
   "source": [
    "The below block of code takes ~10min to run. Hence it is commented out for now, but below are the ideal parameters found after running it last time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fmeCKqJcMDEY",
   "metadata": {
    "id": "fmeCKqJcMDEY"
   },
   "source": [
    "We see that the ideal parameter values are C = `100`, epsilon = `0.1`, gamma= `'auto'`, and kernel = `rbf`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i-WT1aJBMDEX",
   "metadata": {
    "id": "i-WT1aJBMDEX"
   },
   "outputs": [],
   "source": [
    "# # Define the grid of parameters to search\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'C': [0.5, 1, 10, 100], # Note: Values of C > 0\n",
    "#         'gamma':['scale', 'auto', 1, 0.1, 0.001, 0.0001],\n",
    "#         'epsilon': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "#         'kernel': ['rbf'] # 'linear' was also previously added, but since gridsearch computed rbf as optimal, we will remove 'linear' to reduce future run time.\n",
    "#     },\n",
    "# ]\n",
    "# # We are including C=1 and gamma=0.1 since they are the default values\n",
    "\n",
    "# # Initialize the grid search\n",
    "# grid_search = GridSearchCV(model,\n",
    "#                            param_grid,\n",
    "#                            cv=kf, # cross-validation splitting strategy. the function will use the k-fold cross-validation strategy defined by the kf object.\n",
    "#                            scoring='neg_mean_squared_error',\n",
    "#                            n_jobs=-1) # n_jobs, the higher number, more messages\n",
    "\n",
    "# # Conduct the grid search\n",
    "# grid_search.fit(X_encoded, y)\n",
    "\n",
    "# print(grid_search.best_params_) # print the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3HR0fLH0MIZn",
   "metadata": {
    "id": "3HR0fLH0MIZn"
   },
   "source": [
    "## Building final support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o0sl-t7dMIZ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o0sl-t7dMIZ8",
    "outputId": "df2bca35-ae0c-4134-94dd-205993febb9c"
   },
   "outputs": [],
   "source": [
    "# Initialize the SVR model\n",
    "model = SVR(kernel='rbf', C = 100, epsilon = 0.1, gamma = 'auto')\n",
    "\n",
    "# Create an empty list to store scores for each fold\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 0.1  # 10% acceptable limit of error (the percentage of the true value that you'll allow as error)\n",
    "\n",
    "for i in range(kf.get_n_splits()):\n",
    "    # Get the data for this fold\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "\n",
    "    # Fit the model and make predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and store the performance metrics\n",
    "    r2_scores.append(r2_score(y_test, y_pred))\n",
    "    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    # computing accuracy\n",
    "    relative_error = np.abs((y_test - y_pred) / y_test) # Compute the relative error\n",
    "    correct_predictions = np.sum(relative_error <= threshold) # Count the number of predictions that fall within the threshold\n",
    "    accuracy_scores.append(correct_predictions / len(y_test)) # Compute and store the accuracy for this fold\n",
    "\n",
    "# Print average performance metrics\n",
    "print(\"Average R^2 Score:\", np.mean(r2_scores))\n",
    "print(\"Average RMSE:\", np.mean(rmse_scores))\n",
    "print('Average Accuracy: ', np.mean(accuracy_scores)) # Print average accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oc9AyxAxMIZ9",
   "metadata": {
    "id": "oc9AyxAxMIZ9"
   },
   "source": [
    "Without tuning:  R^2 = `0.889`, RMSE = `0.372`, Accuracy = `85.1%`\n",
    "\n",
    "Aftering tuning: R^2 = `0.892`, RMSE = `0.364`, Accuracy =  `85.1%`\n",
    "\n",
    "The SVM preformed really well straight out of the box, with only slight improvements being added to the R^2 and RSME after optimization, but no changce in accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oqFHZpEfRke_",
   "metadata": {
    "id": "oqFHZpEfRke_"
   },
   "source": [
    "## Evaluation: How close is our model predictions to actual 2023 data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I7VrSb_aSJPo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "I7VrSb_aSJPo",
    "outputId": "3a33b1b7-692c-41ab-98ef-c3557202bf28"
   },
   "outputs": [],
   "source": [
    "# Predict the 2023 data using the trained model\n",
    "y_pred_2023 = model.predict(scaler.transform(X_encoded_2023))\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(y_2023, y_pred_2023, alpha=0.5)\n",
    "plt.xlabel('Actual happiness scores from 2023')\n",
    "plt.ylabel('Predicted happiness scores by SVM model')\n",
    "plt.title('Predicted 2023 Happiness Scores as a Function of Actual Scores')\n",
    "\n",
    "# Line of equality\n",
    "plt.plot([min(y_2023), max(y_2023)], [min(y_2023), max(y_2023)], color='red')\n",
    "\n",
    "# annotate\n",
    "avg_r2 = np.mean(r2_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "plt.annotate(f'R^2 = {avg_r2:.2f}\\nRMSE = {avg_rmse:.2f}', xy=(0.75, 0.05), xycoords='axes fraction')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y4LfEthyd8yD",
   "metadata": {
    "id": "y4LfEthyd8yD"
   },
   "source": [
    "1. The points fall along the line of equality. The closer the points are to this line, the more accurate the model's predictions are.\n",
    "2. There is a tight cluster of points around the line of equality, implying a more precise model.\n",
    "3. There is no systematic pattern to the points deviating from the line of equality (for example, if points for lower actual values are consistently overpredicted), this could indicate that the model does not have a bias / captures the relationship between the predictors and the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jm9cdPQu1Pbj",
   "metadata": {
    "id": "Jm9cdPQu1Pbj"
   },
   "source": [
    "## Further performance evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FZcLc3hFjBAM",
   "metadata": {
    "id": "FZcLc3hFjBAM"
   },
   "source": [
    "### Does our prediction model have a lot of error?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nhhs9qUtkHR4",
   "metadata": {
    "id": "nhhs9qUtkHR4"
   },
   "source": [
    " visualizes the errors in out predictions. Ideally, residuals should be randomly and evenly distributed around the x-axis, and shouldn't form any discernible pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lc7UqReujJfV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Lc7UqReujJfV",
    "outputId": "7182ed91-eaff-4ef1-f22b-7d58390d6d73"
   },
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8vunfBcgjY8X",
   "metadata": {
    "id": "8vunfBcgjY8X"
   },
   "source": [
    "### How much are our predictions off by?\n",
    "**What does the distribution mean in plain words?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hFwTn_2Djeha",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "hFwTn_2Djeha",
    "outputId": "824228d6-859b-440a-fee2-2895c6817f26"
   },
   "outputs": [],
   "source": [
    "sns.histplot(residuals, kde=True)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kE2JnMiegL9v",
   "metadata": {
    "id": "kE2JnMiegL9v"
   },
   "source": [
    "* Hard to predict the future! Just because our model performed well on 2023 data, there is no guarantee it will perform as well on the next year's data. The underlying relationships between variables might change over time. A model can make predictions for 2024, but we won't be able to validate these predictions against actual values as we did for the previous years, because the real 2024 data is not available yet.\n",
    "\n",
    "* If there are large year-to-year fluctuations in happiness scores or the predictors, the model might not produce accurate predictions for 2024.\n",
    "* the predictions are essentially a replication of 2023's data with some minor alterations due to the model. They don't account for potential new information or significant changes that may occur in 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3jYUgYmmjvX",
   "metadata": {
    "id": "c3jYUgYmmjvX",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **Q2:** Can we predict 2024 happiness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sar9XfrUg1hm",
   "metadata": {
    "id": "sar9XfrUg1hm"
   },
   "source": [
    "Since our model performed so well, it is reasonable to try an use to forecast the upcoming year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g5kKj1KRIERJ",
   "metadata": {
    "id": "g5kKj1KRIERJ"
   },
   "source": [
    "## Format data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5_f1b9LKByT3",
   "metadata": {
    "id": "5_f1b9LKByT3"
   },
   "source": [
    "To avoid data leakage, we need to split our data into training and test sets before scale our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ncvSteTbzLk4",
   "metadata": {
    "id": "ncvSteTbzLk4"
   },
   "source": [
    "### Split the data into dependent and independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Gen_ageBZif",
   "metadata": {
    "id": "5Gen_ageBZif"
   },
   "source": [
    "Where as in the 2023 model, the training and test dataset did not include 2023 data. In this model, 2023 data is included too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R_zuaytoIOCw",
   "metadata": {
    "id": "R_zuaytoIOCw"
   },
   "outputs": [],
   "source": [
    "# we will make a copy of our df so as to not modify the original.\n",
    "X = df.drop('happiness', axis=1).copy()\n",
    "y = df['happiness'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4MUfz0-ek7qv",
   "metadata": {
    "id": "4MUfz0-ek7qv"
   },
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o6SU0p71k7q5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "o6SU0p71k7q5",
    "outputId": "5c8cdab9-af41-4ea9-92a3-44bb9d278f39"
   },
   "outputs": [],
   "source": [
    "X_encoded = pd.get_dummies(X, columns=['country'])\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CQeajNpNFqRr",
   "metadata": {
    "id": "CQeajNpNFqRr"
   },
   "source": [
    "### Splitting into training and test set & scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1LfNRhVrQWtc",
   "metadata": {
    "id": "1LfNRhVrQWtc"
   },
   "outputs": [],
   "source": [
    "# lets create folds and tell the KFold function whether or not to shuffle the data beforehand.\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Prepare empty lists to hold the training and testing data for each fold\n",
    "X_train_list, X_test_list = [], []\n",
    "y_train_list, y_test_list = [], []\n",
    "\n",
    "for train_index, test_index in kf.split(X_encoded):\n",
    "    # Split the data\n",
    "    X_train, X_test = X_encoded.iloc[train_index], X_encoded.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit the scaler on the training set and transform both sets\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Append to the lists\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OdAxws60khnj",
   "metadata": {
    "id": "OdAxws60khnj"
   },
   "source": [
    "## Building support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-coe6XV7kluD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-coe6XV7kluD",
    "outputId": "4a05f12f-56f6-49cc-b550-65448f6e9e53"
   },
   "outputs": [],
   "source": [
    "# Initialize the SVR model\n",
    "model = SVR(kernel='rbf', C = 100, epsilon = 0.1, gamma = 'auto')\n",
    "\n",
    "# Create an empty list to store scores for each fold\n",
    "r2_scores = []\n",
    "rmse_scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "# Define the threshold\n",
    "threshold = 0.1  # 10% acceptable limit of error (the percentage of the true value that you'll allow as error)\n",
    "\n",
    "for i in range(kf.get_n_splits()):\n",
    "    # Get the data for this fold\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i]\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i]\n",
    "\n",
    "    # Fit the model and make predictions\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and store the performance metrics\n",
    "    r2_scores.append(r2_score(y_test, y_pred))\n",
    "    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "    # computing accuracy\n",
    "    relative_error = np.abs((y_test - y_pred) / y_test) # Compute the relative error\n",
    "    correct_predictions = np.sum(relative_error <= threshold) # Count the number of predictions that fall within the threshold\n",
    "    accuracy_scores.append(correct_predictions / len(y_test)) # Compute and store the accuracy for this fold\n",
    "\n",
    "# Print average performance metrics\n",
    "print(\"Average R^2 Score:\", np.mean(r2_scores))\n",
    "print(\"Average RMSE:\", np.mean(rmse_scores))\n",
    "print('Average Accuracy: ', np.mean(accuracy_scores)) # Print average accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jS_XMJizk4Tq",
   "metadata": {
    "id": "jS_XMJizk4Tq"
   },
   "source": [
    " R^2 = `0.898`, RMSE = `0.358`, Accuracy =  `86.3%`\n",
    "\n",
    "The SVM preformed really well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rhPsNritwUNv",
   "metadata": {
    "id": "rhPsNritwUNv"
   },
   "source": [
    "## Preparing X variables for 2024 forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kQEbBAYYwa8q",
   "metadata": {
    "id": "kQEbBAYYwa8q"
   },
   "source": [
    "After training our Support Vector Machine model on the data from 2005-2023, we will use this trained model to forecast the 'happiness' score (y) for the year 2024.\n",
    "\n",
    "However, in order to make this prediction, we will first need the predictor variables (X) for the year 2024.\n",
    "\n",
    "Since the predictor variables for 2024 are time-independent, we can simply extract the most recent available data for those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gx11SxNSwZiI",
   "metadata": {
    "id": "gx11SxNSwZiI"
   },
   "outputs": [],
   "source": [
    "X_2024 = df[df['year'] == 2023] # Get the most recent data for each country\n",
    "X_2024 = X_2024.drop('happiness', axis=1) # Remove the 'happiness' column, as it's the target variable\n",
    "\n",
    "# we need to make sure X_2024 is processed in the same way as our training data (2005-2023)\n",
    "\n",
    "# One-hot encode the 'country' column to match the training data\n",
    "X_2024_encoded = pd.get_dummies(X_2024, columns=['country'])\n",
    "\n",
    "# Ensure that the 2024 data has the same columns as the training data\n",
    "missing_cols = set(X_encoded.columns) - set(X_2024_encoded.columns)\n",
    "for c in missing_cols:\n",
    "    X_2024_encoded[c] = 0\n",
    "\n",
    "# Make sure the columns are in the same order as in the training data\n",
    "X_2024_encoded = X_2024_encoded[X_encoded.columns]\n",
    "\n",
    "# Scale the 2024 data using the same scaler object used on the training data\n",
    "X_2024_scaled = scaler.transform(X_2024_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v49ymkyRqxGw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "v49ymkyRqxGw",
    "outputId": "5ca1fd9a-6133-40e1-b01f-936b0d7a9f1f"
   },
   "outputs": [],
   "source": [
    "# use the fitted model to predict 2024's happiness scores\n",
    "y_pred_2024 = model.predict(X_2024_scaled)\n",
    "\n",
    "# Create a bar plot for the predicted 2024 happiness scores\n",
    "plt.figure(figsize=(25, 5))\n",
    "plt.scatter(X_2024['country'], y_pred_2024)\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Predicted happiness score in 2024')\n",
    "plt.title('Predicted 2024 Happiness Scores for Each Country')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ypI6pSnr0lfl",
   "metadata": {
    "id": "ypI6pSnr0lfl"
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the country names, the 2023 values, and the predicted 2024 values\n",
    "df_2024 = pd.DataFrame({\n",
    "    'country': X_2024['country'],\n",
    "    '2023_value': df[df['year'] == 2023]['happiness'],  # Get 2023 happiness values from the original dataframe\n",
    "    'predicted_2024_value': y_pred_2024\n",
    "})\n",
    "\n",
    "# Calculate the percent change from 2023 to 2024\n",
    "df_2024['percent_change'] = ((df_2024['predicted_2024_value'] - df_2024['2023_value']) / df_2024['2023_value']) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328zRgMY47Y7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "328zRgMY47Y7",
    "outputId": "46c2dc0c-a344-4669-ae7a-617bebb3afe8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 30))\n",
    "sns.barplot(data=df_2024.sort_values('percent_change', ascending=False), y='country', x='percent_change')\n",
    "plt.xlabel('Percent Change')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Percent Change in Happiness from 2023 to Predicted 2024')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gLoqiBUu4su-",
   "metadata": {
    "id": "gLoqiBUu4su-"
   },
   "source": [
    "The plot is quite large, and if we scaled it down the text will no longer be legible. Instead, we will create two subplots to represent percentage change from 2023 to predicted 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KZf_sSay48iR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KZf_sSay48iR",
    "outputId": "b992879c-c45a-40a0-cf35-7e6f8b063595"
   },
   "outputs": [],
   "source": [
    "# Split the data into positive and negative percent changes\n",
    "df_2024_positive = df_2024[df_2024['percent_change'] > 0].sort_values('percent_change', ascending=False)\n",
    "df_2024_negative = df_2024[df_2024['percent_change'] < 0].sort_values('percent_change')\n",
    "\n",
    "# Create a subplot with two plots\n",
    "fig, axs = plt.subplots(nrows=2, figsize=(10, 20))\n",
    "\n",
    "# Create the bar plot for positive percent changes\n",
    "sns.barplot(data=df_2024_positive, y='country', x='percent_change', ax=axs[0], color='lightgreen')\n",
    "axs[0].set_xlabel('Change in happiness (%)')\n",
    "axs[0].set_ylabel('Country')\n",
    "axs[0].set_title('Positive Percent Change in Happiness from 2023 to Predicted 2024')\n",
    "\n",
    "# Create the bar plot for negative percent changes\n",
    "sns.barplot(data=df_2024_negative, y='country', x='percent_change', ax=axs[1], color='salmon')\n",
    "axs[1].set_xlabel('Change in happiness (%)')\n",
    "axs[1].set_ylabel('Country')\n",
    "axs[1].set_title('Negative Percent Change in Happiness from 2023 to Predicted 2024')\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BQTlPmRkgJIv",
   "metadata": {
    "id": "BQTlPmRkgJIv"
   },
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w03MJZ25fBs-",
   "metadata": {
    "id": "w03MJZ25fBs-",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#  **Q3:** Which variables are most strongly associated with happiness?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CsSR_0BR9Qbq",
   "metadata": {
    "id": "CsSR_0BR9Qbq"
   },
   "source": [
    "Looking at the forecast for global happiness in the upcoming year is interesting. For an example, the UAE is expected to witness the largest positive change in their happiness compared to other countries.\n",
    "\n",
    "What would be even more interesting, is to investigate what are the most salient features that associated with predicting happiness on a global scale. What about the UAE, are the variables that best predict happiness for the UAE population different than remaining countries?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HgePPhbl8MOT",
   "metadata": {
    "id": "HgePPhbl8MOT"
   },
   "source": [
    "Support Vector Machines are not typically used for feature selection or understanding feature importance because they don't provide direct access to which features are most important. This is because SVMs are more of a \"black box\" model where interpretability is difficult.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xw50nEkenIpT",
   "metadata": {
    "id": "Xw50nEkenIpT"
   },
   "source": [
    "*   Are there any surprising variables that have a significant effect on happiness?\n",
    "\n",
    "Knowing what variables contribute to citizens' happiness can help governments develop and prioritize policies that enhance those factors. For instance, if variables related to health and education have strong correlations with happiness, then investing more in public health and education policies could lead to a happier population.\n",
    "\n",
    "Understanding happiness can be important for healthcare providers too. Unhappiness can be both a symptom and a cause of poor health, so healthcare policies might benefit from considering happiness levels in the population.\n",
    "\n",
    "Traditional economic measures like GDP have long been used as a proxy for societal success, but they don't capture everything that matters to people's lives. A happiness model could provide an additional measure to guide economic decision-making.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U9JjW0G-AL3X",
   "metadata": {
    "id": "U9JjW0G-AL3X"
   },
   "source": [
    "## Which variables have the largest influence on determining happiness? **EDIT comments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wcd2S0-RAaBG",
   "metadata": {
    "id": "Wcd2S0-RAaBG"
   },
   "source": [
    "We can fit a simple linear regression model to our data and create a plot of residuals (the difference between the observed and predicted values) versus the predicted values. If we see patterns in this plot, it suggests that the relationship between the predictors and the target variable is not entirely linear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7hN4WSs_DabT",
   "metadata": {
    "id": "7hN4WSs_DabT"
   },
   "source": [
    "Permutation Importance is a great way to measure the importance of different features in your model. You can use this approach to help interpret your model and understand which features are driving the predictions.\n",
    "\n",
    "The idea behind Permutation Importance is simple: for each feature, we randomly shuffle the values of that feature and measure how much the model's performance drops. If shuffling a feature's values leads to a big drop in the model's performance, that feature is important; if there's little change, the feature is not important.\n",
    "\n",
    "You would typically use Permutation Importance on your validation or test data, because it can help you understand which features your model is actually using to make predictions on unseen data. It would be less informative to use Permutation Importance on the training data, because it wouldn't give you insight into how the model performs on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_ozKHA-OfGXi",
   "metadata": {
    "id": "_ozKHA-OfGXi"
   },
   "source": [
    "**Permutation Importance**\n",
    "This technique can help you determine the most influential features in your SVM model by permuting the values of each feature and scoring the impact on model performance. This could be done using sklearn's permutation_importance().\n",
    "\n",
    "Permutation importance is a useful tool for understanding the impact of a specific feature on the predictions of a machine learning model like SVM. It works by permuting the values of each feature one by one and measuring the decrease in the model's performance. The features that cause the biggest drop in performance are considered the most important.\n",
    "\n",
    "However, there are a few things to keep in mind when using permutation importance:\n",
    "\n",
    "Correlated Features: If two features are highly correlated, permuting one of them might not lead to a big drop in performance because the model can still use the other feature to make predictions. In this case, the importance of both features might be underestimated.\n",
    "Interactions between Features: Permutation importance measures the importance of each feature in isolation and does not take into account interactions between features. If the effect of one feature depends on the value of another feature, this interaction effect won't be captured by permutation importance.\n",
    "Non-linear and complex relationships: SVMs and other complex models can capture non-linear relationships and interactions between features, which may not be reflected entirely by permutation importance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pvRzvZHZJS0W",
   "metadata": {
    "id": "pvRzvZHZJS0W"
   },
   "source": [
    "[Reference](https://medium.com/@T_Jen/feature-importance-for-any-model-using-permutation-7997b7287aa) used to write this code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2r_cDj3M2KG",
   "metadata": {
    "id": "s2r_cDj3M2KG"
   },
   "source": [
    "In this case, taking an average of the training sets doesn't make much sense due to the nature of cross-validation. Each set of training data is different because of the shuffling of data, hence it's not advisable to average them.\n",
    "\n",
    "Instead of averaging the training sets, you should compute the permutation importance on each fold of your cross-validation and then average those importances.\n",
    "\n",
    "Please note that permutation importance should ideally be calculated on the validation set or unseen data to provide an unbiased estimate of the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dnc50kY-SdG_",
   "metadata": {
    "id": "dnc50kY-SdG_"
   },
   "source": [
    "In this code, we calculate the permutation importances for each split in the KFold cross-validation. Then we store these importances in a DataFrame, which allows us to easily calculate the average importance for each feature. Finally, we sort the DataFrame by the average importance to easily see which features are most important. Please note, features with small or zero weights might not be important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFV6g4BdTOzk",
   "metadata": {
    "id": "XFV6g4BdTOzk"
   },
   "source": [
    "Data leakage can occur when information that would not be available at prediction time is used when building the model. This can lead to overly optimistic performance estimates.\n",
    "\n",
    "In the provided code, there doesn't seem to be obvious data leakage. You are fitting the model on the training set and then computing permutation importances on the test set, which is the right way to do it.\n",
    "\n",
    "However, the concept of data leakage with Permutation Importance is a nuanced one. If you calculate Permutation Importance using the test set (as you are doing), it can provide insight into what features were most important for generalizing from the training set to unseen data. But, it could be argued that this is a form of data leakage because you're using the test set to influence your interpretation of the model.\n",
    "\n",
    "From a strict perspective, the test set should only be used for the final evaluation, and any form of analysis or interpretation of the model (including Permutation Importance) should be done on a separate validation set or using cross-validation on the training set. This prevents any knowledge of the test set from influencing the model or interpretation.\n",
    "\n",
    "Therefore, while your current approach isn't incorrect, it could be made more robust by calculating Permutation Importance on a separate validation set or using a cross-validation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WkO8PVFiUNNe",
   "metadata": {
    "id": "WkO8PVFiUNNe"
   },
   "source": [
    " To compute permutation importance on validation sets during cross-validation, we can use the same logic and integrate it into our cross-validation loop.\n",
    "\n",
    " This way, we are not using our test set for the calculation of permutation importance but rather using the validation sets from the cross-validation. Note that the `X_test` and `y_test` are essentially acting as validation sets here, as they are different subsets of our training data in each iteration of the cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iZqZDU9ZMM9D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZqZDU9ZMM9D",
    "outputId": "d3e6f916-1e54-49e1-9abc-a76706bff39e"
   },
   "outputs": [],
   "source": [
    "# expect run time to take long (~5-8 minutes)\n",
    "\n",
    "# Initialize a dataframe to store importances for each fold\n",
    "importances_df = pd.DataFrame()\n",
    "\n",
    "# Loop over each split\n",
    "for i in range(kf.get_n_splits()):\n",
    "\n",
    "    # Get the data for this fold\n",
    "    X_train = X_train_list[i]\n",
    "    X_test = X_test_list[i] # In this case, X_test acts as a validation set\n",
    "    y_train = y_train_list[i]\n",
    "    y_test = y_test_list[i] # And y_test acts as validation labels\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Compute the permutation importances\n",
    "    perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\n",
    "\n",
    "    # Get feature names\n",
    "    feature_names = X_encoded.columns.tolist()\n",
    "\n",
    "    # Get importances and convert them into a dataframe\n",
    "    importances = pd.DataFrame(perm.feature_importances_, index=feature_names, columns=[f'Importance Fold {i}'])\n",
    "\n",
    "    # Concatenate the current fold's importances with the previous ones\n",
    "    importances_df = pd.concat([importances_df, importances], axis=1)\n",
    "\n",
    "# Compute the average importances\n",
    "importances_df['Average Importance'] = importances_df.mean(axis=1)\n",
    "\n",
    "# Sort the importances\n",
    "importances_df = importances_df.sort_values('Average Importance', ascending=False)\n",
    "\n",
    "# Print the importances\n",
    "print(importances_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QKt0ZTsiR_WQ",
   "metadata": {
    "id": "QKt0ZTsiR_WQ"
   },
   "source": [
    "Including the 'country' as a feature in our model can provide it with additional information about cultural, social, economic, and political differences between countries, which are not captured by the other predictors in our dataset.\n",
    "\n",
    "However, it's not always straightforward to interpret the effect of the 'country' feature in our model. For instance, 'country_Singapore' having a positive importance means that, all other things being equal, being in Singapore is associated with a higher predicted happiness score compared to the reference country (which, in one-hot encoding, is typically implicitly defined as the country left out of the encoding, or a sort of \"average\" country effect).\n",
    "\n",
    "Also, it's important to note that 'country_Singapore' having a non-zero importance doesn't mean that Singapore is more important to global happiness than other countries. Rather, it means that Singapore's happiness score is more different from the reference country's score than other countries are. Other countries may have zero or near-zero importances because their happiness scores are very similar to the reference country's score.\n",
    "\n",
    "The importance of a feature reflects how much the model's predictions change when you shuffle the values of that feature, thereby destroying the relationship between the feature and the target variable. If the model's predictions change a lot, then the feature is important; if they don't change much, the feature is not important. This method of computing feature importance is robust and can capture complex, nonlinear relationships between features and the target variable.\n",
    "\n",
    "As such, including the 'country' as a feature might be beneficial for our model if there are country-specific factors that affect happiness and are not captured by our other features. On the other hand, if our other features already capture all of the relevant country-specific factors, then including 'country' as a feature might not improve our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FiP6WTmrXyWV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "id": "FiP6WTmrXyWV",
    "outputId": "53540c6f-6b65-4451-c98b-e8837dd9fa0d"
   },
   "outputs": [],
   "source": [
    "importances_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cIFnGymmYkjP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIFnGymmYkjP",
    "outputId": "91139a50-5c94-4592-dc62-a6da4eb3b920"
   },
   "outputs": [],
   "source": [
    "print(importances_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hAE19rxZZEuI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hAE19rxZZEuI",
    "outputId": "09fd3477-aeae-44f9-d2a8-5b8c776ebb2c"
   },
   "outputs": [],
   "source": [
    "importances_df = importances_df[['Average Importance']]\n",
    "importances_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xccV4gDFYRpT",
   "metadata": {
    "id": "xccV4gDFYRpT"
   },
   "source": [
    "We want to create a visualization, but we want the countries to have different color than the other variables i.e., GDP, for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "giexcFAgagS5",
   "metadata": {
    "id": "giexcFAgagS5"
   },
   "outputs": [],
   "source": [
    "# The pandas function str.contains can be used to create a boolean mask indicating\n",
    "# whether each index contains the string 'country'. This mask can then be used to filter the DataFrame.\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = importances_df.index.str.contains('country')\n",
    "\n",
    "# Create DataFrame for features containing 'country'\n",
    "df_importanceCountry = importances_df[mask]\n",
    "\n",
    "# Create DataFrame for features not containing 'country'\n",
    "df_importanceNon_country = importances_df[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NpnDIYX7cav6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NpnDIYX7cav6",
    "outputId": "6b2d24d8-d6ad-4948-c543-7aff24b99cf2"
   },
   "outputs": [],
   "source": [
    "# Remove 'country_' from the index\n",
    "df_importanceCountry.index = df_importanceCountry.index.str.replace('country_', '', regex=False)\n",
    "df_importanceCountry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lpc0ueGwZSYX",
   "metadata": {
    "id": "lpc0ueGwZSYX"
   },
   "outputs": [],
   "source": [
    "# lets create a color map\n",
    "def color_negative_red(value):\n",
    "  \"\"\"\n",
    "  Colors values in a DataFrame green if they are positive and red if they are negative.\n",
    "\n",
    "  Args:\n",
    "    value (float): The value to color.\n",
    "\n",
    "  Returns:\n",
    "    str: The color string.\n",
    "  \"\"\"\n",
    "  if value < 0:\n",
    "    color = 'red'\n",
    "  else:\n",
    "    color = 'green'\n",
    "\n",
    "  return 'color: %s' % color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z6VCb-Rycxco",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "z6VCb-Rycxco",
    "outputId": "945d40ee-1bf1-4122-b844-c32f303ee327"
   },
   "outputs": [],
   "source": [
    "# Apply the color map to dataframe for feature imoportance by country\n",
    "styledCountry_df = df_importanceCountry[['Average Importance']].style.applymap(color_negative_red)\n",
    "\n",
    "styledCountry_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YduSpIoLbKIw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "YduSpIoLbKIw",
    "outputId": "5fc25f11-45a9-4c6a-b58f-b2d383abe629"
   },
   "outputs": [],
   "source": [
    "# Apply the color map to dataframe for feature imoportance by scores\n",
    "styledScores_df = df_importanceNon_country[['Average Importance']].style.applymap(color_negative_red)\n",
    "\n",
    "styledScores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dv3p3yQQhs5P",
   "metadata": {
    "id": "dv3p3yQQhs5P"
   },
   "source": [
    "## Are there any interacting variables that predict happiness?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3XS_1U4xhzOw",
   "metadata": {
    "id": "3XS_1U4xhzOw"
   },
   "source": [
    "Pairplots can be used to visualize potential interactions between features in relation to the target variable. To create this we just need  a 3D plot that can illustrate the relationship between two features and the target variable (happiness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prolmNk6iV_6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prolmNk6iV_6",
    "outputId": "0376b2ed-73a9-4282-9341-38315e8e7e32"
   },
   "outputs": [],
   "source": [
    "# the feature names are from this df\n",
    "print(X_encoded.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0C5qnKeEilHX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C5qnKeEilHX",
    "outputId": "c9ff433b-f3f2-436e-ba27-ff8455546045"
   },
   "outputs": [],
   "source": [
    "# what are all the possible combinations of features that we can plot?\n",
    "# the features we are interested in are: GDP, socialSupport, corruption, freedom, generosity, year,and, lifeExpectancy\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "features = ['GDP', 'socialSupport', 'corruption', 'freedom', 'generosity', 'year', 'lifeExpectancy']\n",
    "\n",
    "# Get all possible combinations of 2 features\n",
    "feature_combinations = list(combinations(features, 2))\n",
    "\n",
    "# let's print them\n",
    "for combo in feature_combinations:\n",
    "    print(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tkljphq_jOL9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkljphq_jOL9",
    "outputId": "c15d17bc-d24e-4344-a784-a1733ed15655"
   },
   "outputs": [],
   "source": [
    "# above we found all the possible combinations. So how many combination in total can we plot?\n",
    "print(len(feature_combinations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFV0DmiGjWgy",
   "metadata": {
    "id": "oFV0DmiGjWgy"
   },
   "source": [
    "We have 21 possible plots to make. We could manually make a plot for each pair, but lets make our life easier and just use a loop to do it all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BUN6Pw3WnDHS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BUN6Pw3WnDHS",
    "outputId": "9da4582e-da9f-4fae-dc49-c17444619896"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from itertools import combinations\n",
    "\n",
    "selected_features = ['GDP', 'socialSupport', 'corruption', 'freedom', 'generosity', 'year', 'lifeExpectancy']\n",
    "\n",
    "# Get all possible combinations of 2 features from the selected features\n",
    "selected_feature_combinations = list(combinations(selected_features, 2))\n",
    "\n",
    "# we already fit the model and made predictions earlier for research question #2 called 'y_pred'\n",
    "y_pred_full = model.predict(X_encoded)\n",
    "\n",
    "for pair in selected_feature_combinations:\n",
    "    feature1, feature2 = pair\n",
    "\n",
    "    # Create a new figure\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "\n",
    "    # Add a 3D subplot\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Create a scatter plot with 'feature1' and 'feature2' as the x and y axes, and the predicted target as the z axis\n",
    "    ax.scatter(X_encoded[feature1], X_encoded[feature2], y_pred_full)\n",
    "\n",
    "    # Set labels for the axes\n",
    "    ax.set_xlabel(feature1)\n",
    "    ax.set_ylabel(feature2)\n",
    "    ax.set_zlabel('Predicted Happiness')\n",
    "\n",
    "    # Set title for the plot\n",
    "    ax.set_title(f'3D scatter plot for features {feature1} and {feature2}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sqmnNm_ypV47",
   "metadata": {
    "id": "sqmnNm_ypV47"
   },
   "source": [
    "These plots are static, so it is hard to visually inspect. Let's make an interactive plot instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bgwO9BkupT2I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bgwO9BkupT2I",
    "outputId": "37d3eb0a-2aa7-44e2-d585-6bcc3e6a6530"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from itertools import combinations\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.renderers.default='colab' # enables 3D interactive plots in Google Colab using plotly\n",
    "\n",
    "# Loop through the pairs of features\n",
    "for pair in selected_feature_combinations:\n",
    "    feature1, feature2 = pair\n",
    "\n",
    "    trace = go.Scatter3d(\n",
    "        x = X_encoded[feature1],\n",
    "        y = X_encoded[feature2],\n",
    "        z = y_pred_full,\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 4,\n",
    "            color = y_pred_full,\n",
    "            colorscale = 'Viridis'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    data = [trace]\n",
    "    layout = go.Layout(\n",
    "        title = f'3D scatter plot for features {feature1} and {feature2}',\n",
    "        scene = dict(\n",
    "            xaxis = dict(title = feature1),\n",
    "            yaxis = dict(title = feature2),\n",
    "            zaxis = dict(title = 'Predicted Happiness')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_flHZ5SzC_Qd",
   "metadata": {
    "id": "_flHZ5SzC_Qd"
   },
   "source": [
    "# Focusing on the Governance-Quality Measures based on Data from the Worldwide Governance Indicators (WGI) Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wv0rESnbzM_a",
   "metadata": {
    "id": "Wv0rESnbzM_a"
   },
   "source": [
    "# Pending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E9jAEIQucGj3",
   "metadata": {
    "id": "E9jAEIQucGj3"
   },
   "source": [
    "## Scatter diagram of the distribution between each parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37p2RwvEaSWP",
   "metadata": {
    "id": "37p2RwvEaSWP"
   },
   "source": [
    "## Correlation matrix between all the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZxzVmfuibP_B",
   "metadata": {
    "id": "ZxzVmfuibP_B"
   },
   "source": [
    "why? Correlation is used to identify the pairwise correlation of all columns that exist in the dataset. By using heatmap, it explicitly exhibits the attributes which are strongly correlated to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vjyRtfZZlvDI",
   "metadata": {
    "id": "vjyRtfZZlvDI"
   },
   "source": [
    "## Which country is performing the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1jbhHzAkl7YE",
   "metadata": {
    "id": "1jbhHzAkl7YE"
   },
   "source": [
    "Happiness can be a useful metric for international comparisons. Countries often strive to learn from each other's successes and failures, and understanding the factors that contribute to happiness can provide valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uT1nCN8nl0kS",
   "metadata": {
    "id": "uT1nCN8nl0kS"
   },
   "source": [
    "### Overall happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-6SgRV2wl2rw",
   "metadata": {
    "id": "-6SgRV2wl2rw"
   },
   "source": [
    "### High performers in each independent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1REvD3zbyGx",
   "metadata": {
    "id": "n1REvD3zbyGx"
   },
   "source": [
    "### Heatmap of Global happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cCHtJwZb1UH",
   "metadata": {
    "id": "2cCHtJwZb1UH"
   },
   "source": [
    "Geographical Visualization of Happiness Score: to help to comprehend trends in our data, identify outliers as well as other areas of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3q5m5WAnmDAh",
   "metadata": {
    "id": "3q5m5WAnmDAh"
   },
   "source": [
    "# Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-lHy_jGVmGCO",
   "metadata": {
    "id": "-lHy_jGVmGCO"
   },
   "source": [
    "While our model could be very valuable, it's important to remember that these are complex issues, and that the model's predictions should ideally be one part of a larger decision-making process. It's also crucial to consider factors such as data quality and representation, the potential for overfitting, and ethical considerations.\n",
    "\n",
    "**What are the reason a machine learning model can perform really well, but the dimension in the data doesnt capture complex behaviour.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CZE8jfjYakd_",
   "metadata": {
    "id": "CZE8jfjYakd_"
   },
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "F7ZVKOYsamWj",
   "metadata": {
    "id": "F7ZVKOYsamWj"
   },
   "source": [
    "As a consequence, SVM regression might be used to determine the happiness of a country. If happiness can be predicted at an early stage, it is beneficial to stakeholds to prevent a decline.\n",
    "\n",
    "Maximize life satisfaction."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "N_SdK2fDSvLh",
    "_pdbRUevTEdy",
    "W3Iia0SPzBJm",
    "ScrbBosq_eL_",
    "8X2R17x81ymj",
    "pAnG2TrK0KL1",
    "0hBdHNYxzyyc",
    "sktrs2kpsIIH",
    "jBbBbWQ0Ym_-",
    "JaENk7ZzSqZW",
    "uqfuthBnIQAf",
    "SMbeAUf69rEi",
    "rZ6yS7kJ9rEj",
    "T2sanB9S9rEk",
    "bmF6pM8vL3VJ",
    "klWfhzl1MDEL",
    "3HR0fLH0MIZn",
    "oqFHZpEfRke_",
    "g5kKj1KRIERJ",
    "ncvSteTbzLk4",
    "CQeajNpNFqRr",
    "BBSFcvQ5RH5-",
    "0LdLNCeoSL4G",
    "OdAxws60khnj",
    "6H3ioFXlYDBG",
    "FZcLc3hFjBAM",
    "8vunfBcgjY8X",
    "w03MJZ25fBs-",
    "V7mSgLvOzHng",
    "aYKAmLxIzb8I",
    "Wv0rESnbzM_a",
    "3q5m5WAnmDAh",
    "CZE8jfjYakd_"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
